# -*- coding: utf-8 -*-
"""DL_assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12hF4sLH2w8FRBgCIHgCqnuaou-FAK9xp
"""

!pip install ucimlrepo
from ucimlrepo import fetch_ucirepo

# fetch dataset
covertype = fetch_ucirepo(id=31)

# data (as pandas dataframes)
X = covertype.data.features
y = covertype.data.targets

# metadata
print(covertype.metadata)

# variable information
print(covertype.variables)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# no missing values and things are already encoded

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify non-binary columns
non_binary_cols = []
for col in X_train.columns:
    if len(X_train[col].unique()) > 2:
        non_binary_cols.append(col)

# Feature scaling (standardization) for non-binary columns
scaler = StandardScaler()
X_train[non_binary_cols] = scaler.fit_transform(X_train[non_binary_cols])
X_test[non_binary_cols] = scaler.transform(X_test[non_binary_cols])

X_train

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
X_train = X_train.astype("float32")
X_test = X_test.astype("float32")

nb_classes = 7
y_train=y_train-1
y_test=y_test-1
Y_train = to_categorical(y_train, nb_classes)
Y_test = to_categorical(y_test, nb_classes)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD


# # Define the MLP model
# model = Sequential()
# model.add(Dense(32, activation='relu', input_shape=(54,)))  # Input layer
# model.add(Dense(32, activation='relu'))  # Hidden layer
# model.add(Dense(7, activation='softmax'))  # Output layer (7 classes)

# # Compile the model
# model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])


# # Train the model
# model.fit(X_train, Y_train, epochs=10, batch_size=128, validation_split=0.1)  # Adjust epochs and batch_size as needed

# # Evaluate the model
# loss, accuracy = model.evaluate(X_test, Y_test)
# print(f"Test Loss: {loss:.4f}")
# print(f"Test Accuracy: {accuracy:.4f}")

from tensorflow.keras.layers import Dense, Input, Add, Activation
from tensorflow.keras.models import Sequential, Model

# Input layer
input_tensor = Input(shape=(X_train.shape[1],))

# Initial layers
x = Dense(128, activation='relu')(input_tensor)
x = Dense(64, activation='relu')(x)

# Custom Residual Block
skip = x  # Store input for skip connection
residual = Dense(64, activation='relu')(x)
residual = Dense(64, activation='relu')(residual)

# Linear projection for residual connection (if dimensions differ)
if residual.shape[-1] != skip.shape[-1]:
    skip = Dense(residual.shape[-1])(skip)

residual = Add()([residual, skip])
residual = Activation('relu')(residual)

# Additional Skip Connection
x = Add()([x, residual]) #adding skip connection output to previous layer's output
x = Activation('relu')(x)

# Final Layers
x = Dense(32, activation='relu')(x)
output_tensor = Dense(7, activation='softmax')(x)  # Output layer (7 classes)

# Create the model
model = Model(inputs=input_tensor, outputs=output_tensor)

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, Y_train, epochs=10, batch_size=128, validation_split=0.1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

model.save('my_model.h5')

import numpy as np

# Overfitting Experiment
batch_size = 128
single_batch_indices = np.random.choice(len(X_train), batch_size, replace=False)
X_batch = X_train.iloc[single_batch_indices]
y_batch = Y_train[single_batch_indices]

# Train on a single batch
history = model.fit(X_batch, y_batch, epochs=500, verbose=0) # Increased epochs for near-zero loss

# Validation Check
validation_loss = model.evaluate(X_test, Y_test, verbose=0)[0]
training_loss = history.history['loss'][-1]

# Get the number of parameters
num_params = model.count_params()

# Print the results
print(f"Number of parameters: {num_params}")
print(f"Final training loss: {training_loss:.4f}")
print(f"Final validation loss: {validation_loss:.4f}")

